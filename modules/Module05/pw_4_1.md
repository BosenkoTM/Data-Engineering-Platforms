# Практическая работа 4.1. Запуск dbt-проекта с помощью Airflow DAG в GCP

## Цель работы
Научиться создавать, настраивать и запускать простой Airflow DAG в среде Google Cloud Composer для выполнения команд проекта dbt (data build tool), который преобразует данные непосредственно в BigQuery.

## Задачи
-   **Настроить рабочее окружение.** Подготовить необходимые сервисы в GCP, включая BigQuery, Cloud Composer и Cloud Storage.
-   **Создать и настроить dbt-проект.** Разработать базовый dbt-проект с одной или несколькими моделями для трансформации данных в BigQuery согласно выбранному варианту задания.
-   **Загрузить dbt-проект в Cloud Storage.** Разместить файлы проекта в бакете, доступном для Cloud Composer.
-   **Написать и развернуть Airflow DAG.** Создать Python-скрипт DAG, который будет выполнять команды `dbt run` и `dbt test` для проекта.
-   **Запустить и отмониторить выполнение DAG.** Убедиться, что DAG успешно запускается и корректно выполняет dbt-задачи.

## Необходимое ПО и сервисы
-   **Google Cloud Platform (GCP) Account**
-   **Google Cloud SDK (gcloud CLI)**
-   **Cloud Composer (v2)**
-   **BigQuery**
-   **Cloud Storage**
-   **dbt (data build tool)**
-   **Текстовый редактор** (например, VS Code)

## Ход работы

### Шаг 1. Подготовка окружения в GCP
1.  **Создайте проект в GCP** и активируйте API для `Cloud Composer`, `BigQuery` и `Cloud Storage`.
2.  **Создайте наборы данных в BigQuery:**
    -   `raw_data` — для исходных данных.
    -   `dbt_transformed` — для таблиц, созданных dbt.
3.  **Загрузите исходные данные.** Создайте в `raw_data` таблицу, соответствующую вашему варианту задания, и загрузите в нее данные.
4.  **Создайте среду Cloud Composer 2:**
    -   При создании в разделе "PyPI packages" добавьте `dbt-bigquery`, чтобы DAG мог выполнять команды dbt.
5.  **Найдите GCS бакет.** После создания среды Composer найдите связанный с ней бакет. Он будет использоваться для хранения DAG'ов и файлов dbt-проекта.

### Шаг 2. Создание и настройка dbt-проекта
1.  **Инициализируйте dbt-проект** на локальной машине:
    ```bash
    dbt init my_dbt_project
    ```
2.  **Настройте `profiles.yml`** для подключения к вашему проекту в BigQuery.
3.  **Создайте dbt-модель.**
    -   В папке `models/` создайте `.sql` файл с трансформационной логикой для вашего варианта.
    -   *Пример* для трансформации таблицы `users`:
        ```sql
        -- models/stg_users.sql
        SELECT
          user_id,
          UPPER(name) as name_uppercase
        FROM
          `ваш-gcp-проект-id.raw_data.users`
        ```
4.  **Настройте `dbt_project.yml`**, указав имя вашего профиля.

### Шаг 3. Загрузка dbt-проекта в Cloud Storage
-   Создайте в GCS-бакете вашей среды Composer папку `dags/dbt/`.
-   Загрузите всю папку вашего dbt-проекта (`my_dbt_project`) в созданную директорию. Путь должен выглядеть так: `gs://<composer-bucket>/dags/dbt/my_dbt_project/`.

### Шаг 4. Написание и развертывание Airflow DAG
1.  **Создайте Python-файл** `dbt_dag.py`.
2.  **Напишите код DAG**, который будет запускать dbt-команды.
    ```python
    from airflow import DAG
    from airflow.operators.bash import BashOperator
    from datetime import datetime

    # Путь к dbt-проекту внутри GCS, доступный для Airflow
    DBT_PROJECT_DIR = '/home/airflow/gcs/dags/dbt/my_dbt_project'

    with DAG(
        dag_id='dbt_simple_dag',
        start_date=datetime(2023, 1, 1),
        schedule_interval='@daily',
        catchup=False,
        tags=['dbt'],
    ) as dag:
        # Запуск моделей dbt
        dbt_run = BashOperator(
            task_id='dbt_run',
            # Команда выполняется из директории проекта
            bash_command=f"cd {DBT_PROJECT_DIR} && dbt run"
        )

        # Запуск тестов dbt
        dbt_test = BashOperator(
            task_id='dbt_test',
            bash_command=f"cd {DBT_PROJECT_DIR} && dbt test"
        )

        # Тесты запускаются после выполнения моделей
        dbt_run >> dbt_test
    ```
3.  **Загрузите `dbt_dag.py`** в папку `dags/` вашего GCS-бакета. Airflow автоматически подхватит его.

### Шаг 5. Запуск и мониторинг
1.  **Откройте UI Airflow** из консоли Cloud Composer.
2.  Найдите DAG `dbt_simple_dag` и запустите его вручную.
3.  Проверьте логи выполнения задач `dbt_run` и `dbt_test`.
4.  После успешного выполнения перейдите в BigQuery и убедитесь, что в наборе данных `dbt_transformed` появилась новая таблица с преобразованными данными.

### Выбор варианта задания
Вам необходимо выбрать один из **35 вариантов**, представленных в исходном файле задания. Данная инструкция является универсальным шаблоном. Ваша задача — адаптировать **Шаг 1 (загрузка данных)** и **Шаг 2 (создание dbt-модели)** под выбранную тему и датасет.

[Варианты заданий на образовательном портале](http://95.131.149.21/moodle/mod/assign/view.php?id=2019&forceview=1)

---

# Отчет и критерии оценки

## Требования к отчету
Отчет оформляется в этом файле (`README.md`) и должен включать:

1.  **Цель работы.** Краткое изложение цели.
2.  **Краткое описание решения.** Описание архитектуры реализованного ELT-процесса.
3.  **Структура репозитория.**
    -   `/my_dbt_project/` - папка с dbt-проектом.
    -   `/dags/dbt_dag.py` - файл с кодом Airflow DAG.
    -   `README.md` - данный отчет.
4.  **Инструкция по запуску.** Краткие шаги для развертывания проекта.
5.  **Результаты выполнения.**
    -   Скриншот графа успешно выполненного DAG из веб-интерфейса Airflow.
    -   Скриншот таблицы с преобразованными данными в интерфейсе BigQuery.
6.  **Выводы.** Что было изучено, какие преимущества дает использование связки Airflow + dbt.

## Критерии оценки (10 баллов)

### 1. Настройка dbt-проекта (2 балла)
-   **(1 балл)** Корректно создана и настроена как минимум одна dbt-модель, выполняющая SQL-трансформацию согласно варианту.
-   **(1 балл)** Файл `dbt_project.yml` сконфигурирован правильно.

### 2. Разработка Airflow DAG (3 балла)
-   **(1 балл)** Код DAG написан без синтаксических ошибок и корректно отображается в UI Airflow.
-   **(1 балл)** DAG использует `BashOperator` для корректного вызова команд `dbt run` и `dbt test`.
-   **(1 балл)** Зависимости между задачами (`dbt_run >> dbt_test`) настроены корректно.

### 3. Работоспособность и конечный результат (3 балла)
-   **(1.5 балла)** DAG успешно выполняется от начала до конца без ошибок (подтверждено скриншотом).
-   **(1.5 балла)** В BigQuery успешно создана целевая таблица с данными, соответствующими логике dbt-модели (подтверждено скриншотом).

### 4. Качество отчета и репозитория (2 балла)
-   **(1 балл)** Репозиторий имеет логичную структуру, отчет оформлен в `README.md` и содержит все обязательные разделы.
-   **(1 балл)** Отчет содержит все необходимые скриншоты, а выводы отражают понимание проделанной работы.